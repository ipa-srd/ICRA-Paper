\section{Experimental results}
\label{sec:Experiment}

\subsection{Classification of lidar data}

In order to evaluate the leg detector described in \ref{subs:classification}, we set up our mobile robot\footnote{http://www.care-o-bot.de/de/rob-work.html} equipped with a Sick S300 safety laser scanner\footnote{http://sick.com} in a populated corridor in our department, recording legs of people passing through over a course of three hours. 
For evaluating the accuracy of the leg detector, laser scans points from the static environment were manually removed from the scan, resulting in a data set which only contains measurements resulting from human legs and serving as the ground truth.
%The leg detector receives the input from the laser scan and labels each point as leg or non-leg.
The results are shown in Figure \ref{fig:radius_detection}, where a short, 100 second long data set was analyzed.
We find, that the detector works best in close range, while becoming weaker as the range increases. 

The decrease in the first half originates mainly from the difficulty of interpreting the laser data, as the points spread further out depending on the distance to the scanner. The second half additionally suggests too few data points as input to the convolutional network and the clustering algorithm, which ultimately decides the outcome.
At a distance of $2.2 m$ we still find good results in a reasonable range around the robot, which we can use to evaluate a three hour long recording of the same corridor. We find the confusion matrix in Table \ref{tab:truth}, omitting false and true negatives as the sensor recorded only a specific wall in that range as negative data, making the input redundant. The long recording shows how well the network reacts to more arbitrary data, most importantly the accuracy is still high, which we can compare against the state of the art leg detector.

\begin{figure}[]
	\normalsize
	\begin{center}
		\input{figures/radius_detection.pgf}
	\end{center}
	\caption{\textbf{Distance dependency of the leg detector.} The diagram shows accuracies of the leg detection as a function of the number of points in a range around the laser scanner, which were counted on a frame-by-frame basis.}
	\label{fig:radius_detection}
\end{figure}

\begin{table}[]
	\def \confa {121199}
	\def \confb {19415}
	\FPeval{\confar}{round((1-\confb/\confa)*100,1)}
	\FPeval{\confbr}{round((\confb/\confa)*100,1)}
	\def \confc {4}
	\def \confd {18177714}
	\FPeval{\confcr}{round(1-\confd/\confc,1)}
	\FPeval{\confdr}{round(\confd/\confc,1)}
	\centering
	\caption{Leg detection in a radius of $2.2 m$.}
	\label{tab:truth}
	\begin{tabular}{|l|l|l|l|}
	\hline
 & \multicolumn{2}{c}{Detected Label} &  \\ \hline
 True Label & Leg & Non-Leg & Total \\ \hline
 Leg & \textbf{\confa} ($\confar \%$) & \textbf{\confb} ($\confbr \%$) & \textbf{11750} \\
 %No Leg & \textbf{\confc} ($0.0 \%$) & \textbf{\confd} ($100.0 \%$) & \textbf{292348} \\
 \hline
	\end{tabular}
\end{table}

\subsection{People detection using clusters}

Detecting people from the output of the laser classification is achieved by clustering the results as explained in section \ref{subs:clustering}. We compare the detection to the algorithm provided in the official ROS-repository\footnote{http://wiki.ros.org/leg\_detector} based on the paper of Arras et. al.\cite{Arras07usingboosted}, which uses a Kalman filter to detect people and an Adaboost algorithm to label laser points. A ten second long data set from the corridor data was hand-labeled and compared to the outputs of the algorithm. Due to the dependency on the distance to the sensor, different radii were considered. We find the parameters:

\begin{figure}
	\normalsize
	\begin{center}
		\input{figures/people_det.pgf}
	\end{center}
	\caption{\textbf{Accuracy of people detection.} People detectioin was benchmarked against the official leg detector, while counting the number of people on a frame-by-frame basis. The new convolutional leg detector outperforms the official leg detector in all tests.}
	\label{fig:people_detection}
\end{figure}

\begin{itemize}
	\item \textbf{True positives} ($TP$):\\The people that were detected correctly,
	\item \textbf{False positives} ($FP$):\\Non-human objects classified as people,
	\item \textbf{False negatives} ($FN$):\\People that were not detected,
\end{itemize}
which are combined into the $F-Measure$, separating good results (close to $1$) from bad results (close to $0$).
\begin{equation}
	F=\frac{2*Precision*Recall}{Precision+Recall} \in \left[ 0,1 \right]
\end{equation}
with
\begin{equation}
	Precision = \frac{TP}{TP + FP}\\
\end{equation}
\begin{equation}
	Recall = \frac{TP}{TP + FN}
\end{equation}

The Adaboost leg detector outputs probabilites of persons and legs, therefore the threshold was adapted to compare the results in a more meaningful way. In Figure \ref{fig:people_detection}, we find the detections. Especially interesting is the $Precision$, as we should not allow the robot to detect any false positives. The proposed, convoluted leg detector only starts picking up false positives after $3.4 m$, like the Adaboost leg detector with 70\% threshold. However, when comparing these two, we find a large gap in the recall, meaning that the Adaboost leg detector has a low detection rate.
Conclusively, the convoluted leg detector performs very well in regards to false positives and false negatives leading to a high $Recall$ and a high $Precission$, which can be seen in the $F-Measure$.


\subsection{Trajectory prediction}

From the high accuracy of the people detection, we can extract positions to learn to predict trajectories. A benchmark has not been performed, although we can find the results of the trained network as images in Figure \ref{fig:trajectory_pred}. The algorithm outputs a mixture of normal distributions, which can be extracted to the most likely positions in order to find future locations of people, which are not displayed in the diagram but can be directly modeled from the output densities.

The prediction lies within good accordance of the ground truth and outputs the future trajectory with a high probability. In the next step, the output of the program can be fed into the network again, to produce successive predictions for almost any timespan.

\begin{figure}
	\normalsize
	\begin{center}
		 \includegraphics[width=0.45\textwidth]{figures/trajectory_pred.png}
	\end{center}
	\caption{\textbf{Trajectory prediction.} Current and past positions of the person as an output of the people detection algorithm described in Sec. \ref{subs:clustering} are displayed as grey circles. The normal distributions which resemble the predicted locations of the person are displayed as the heat map in the background. Ground truth position of the actual movement are shown as red cicles.}
	\label{fig:trajectory_pred}
\end{figure}
